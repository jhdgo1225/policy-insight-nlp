"""
KoBART ëª¨ë¸ ì¶”ë¡  ì „ìš© ìŠ¤í¬ë¦½íŠ¸
ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import json
import os
import requests
import re
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# tokenizers ë³‘ë ¬ ì²˜ë¦¬ ì¶©ëŒ ë°©ì§€
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ì „ì—­ ë³€ìˆ˜
_okt_instance = None
_korean_stopwords = None

def get_okt_and_stopwords():
    """ì „ì—­ Okt ì¸ìŠ¤í„´ìŠ¤ì™€ ë¶ˆìš©ì–´ë¥¼ ë°˜í™˜ (JVM ì¶©ëŒ ë°©ì§€)"""
    global _okt_instance, _korean_stopwords
    
    if _okt_instance is None:
        from konlpy.tag import Okt
        _okt_instance = Okt()
        
    if _korean_stopwords is None:
        try:
            import requests
            url = "https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.txt"
            _korean_stopwords = set(requests.get(url).text.split("\n"))
            for stopword in ["ì€", "ëŠ”", "ì´", "ê°€", "ê³ ", "ì •ë§", "ì˜", "ì´ë‚˜", "ì´ë¼ê³ ", "ì¸", "ì´ë‹¤", "í•˜ì—¬", "``", "ì—", "ì—ëŠ”"]:
                _korean_stopwords.add(stopword) 
            _korean_stopwords.remove("ëª¨")
        except:
            # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë¶ˆìš©ì–´ ì‚¬ìš©
            _korean_stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ê³ ", "ì •ë§", "ì˜", "ì´ë‚˜", "ì´ë¼ê³ ", "ì¸", "ì´ë‹¤", "í•˜ì—¬", "``", "ì—", "ì—ëŠ”"}
    
    return _okt_instance, _korean_stopwords


def filter_text(lines):
    """ê¸°ì/ì´ë©”ì¼ í¬í•¨ ë¬¸ì¥ ì œê±°"""
    reporter_pattern = r'[ê°€-í£]{2,4}\s+ê¸°ì'
    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'

    def should_keep(segment):
        """ê¸°ì/ì´ë©”ì¼ì´ ì—†ìœ¼ë©´ True"""
        return (not re.search(reporter_pattern, segment) and 
                not re.search(email_pattern, segment))

    return [line for line in lines if should_keep(line)]


def preprocess_text_for_inference(text_lines):
    """
    ì¶”ë¡ ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    1. ê¸°ì/ì´ë©”ì¼ ì œê±°
    2. í˜•íƒœì†Œ ë¶„ì„ ë° ë¶ˆìš©ì–´ ì œê±°
    """
    # 1. ê¸°ì/ì´ë©”ì¼ ì œê±°
    filtered_lines = filter_text(text_lines)
    
    # 2. í˜•íƒœì†Œ ë¶„ì„ ë° ë¶ˆìš©ì–´ ì œê±°
    okt, stopwords = get_okt_and_stopwords()
    
    processed_lines = []
    for line in filtered_lines:
        # í˜•íƒœì†Œ ë¶„ì„
        morphs = okt.morphs(line)
        # ë¶ˆìš©ì–´ ì œê±°
        filtered_morphs = [word for word in morphs if word not in stopwords]
        processed_line = ' '.join(filtered_morphs)
        processed_lines.append(processed_line)
    
    return processed_lines


def summarize_text(text_input, model, tokenizer, device):
    """
    í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        text_input: ì…ë ¥ í…ìŠ¤íŠ¸ (ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸)
        model: ë¡œë“œëœ KoBART ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
    
    Returns:
        ìš”ì•½ë¬¸ (ë¬¸ìì—´)
    """
    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì „ì²˜ë¦¬ ìˆ˜í–‰
    if isinstance(text_input, list):
        processed_lines = preprocess_text_for_inference(text_input)
        input_text = ' '.join(processed_lines)
    else:
        input_text = text_input
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(
        input_text,
        max_length=1024,
        truncation=True,
        padding='max_length',
        return_tensors='pt'
    )
    
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_length=256,
            num_beams=5,
            early_stopping=True,
            no_repeat_ngram_size=2,
            length_penalty=1.0,
            temperature=1.0
        )
    
    # ë””ì½”ë”©
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*70)
    print(" "*20 + "KoBART í…ìŠ¤íŠ¸ ìš”ì•½ (ì¶”ë¡  ì „ìš©)")
    print("="*70)
    
    # ëª¨ë¸ ë¡œë“œ
    print("\n[1ë‹¨ê³„] ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model_path = './kobart_final_model'
    
    if not os.path.exists(model_path):
        print(f"âŒ ì˜¤ë¥˜: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("   ë¨¼ì € 'kobart_summarization_complete.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”.")
        return
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.eval()  # í‰ê°€ ëª¨ë“œ
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return
    
    # ì˜ˆì‹œ í…ìŠ¤íŠ¸ë¡œ ì¶”ë¡ 
    print("\n[2ë‹¨ê³„] ì˜ˆì‹œ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½ ìˆ˜í–‰\n")
    
    # ì˜ˆì‹œ 1: ë‰´ìŠ¤ ê¸°ì‚¬
    print("-"*70)
    print("[ì˜ˆì‹œ 1] ë‰´ìŠ¤ ê¸°ì‚¬")
    print("-"*70)
    news_text = [
        "ê¹€ì² ìˆ˜ ê¸°ì = ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ê¸‰ê²©íˆ ë°œì „í•˜ë©´ì„œ ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆë‹¤.",
        "íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œëŠ” BERT, GPTì™€ ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ë“±ì¥í–ˆë‹¤.",
        "ì´ë©”ì¼: reporter@example.com",
        "í•œêµ­ì—ì„œë„ SKí…”ë ˆì½¤ì´ KoBART, KoBERT ë“± í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ì„ ê°œë°œí–ˆë‹¤.",
        "ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ë¬¸ì„œ ìš”ì•½, ê°ì„± ë¶„ì„, ì§ˆì˜ì‘ë‹µ ë“±ì— í™œìš©ë˜ê³  ìˆë‹¤.",
        "ê¹€ì˜í¬ ê¸°ì(younghee@news.com)ëŠ” ì´ëŸ¬í•œ ê¸°ìˆ ì´ ë¯¸ë””ì–´ ì‚°ì—…ì—ë„ í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì „ë§í–ˆë‹¤.",
        "ì•ìœ¼ë¡œ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì€ ë”ìš± ì •êµí•´ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤."
    ]
    
    print("\n[ì›ë³¸]")
    for i, line in enumerate(news_text, 1):
        print(f"  {i}. {line}")
    
    summary = summarize_text(news_text, model, tokenizer, device)
    print(f"\n[ìš”ì•½ë¬¸]\n  {summary}\n")
    
    # ì˜ˆì‹œ 2: ê¸°ìˆ  ë¬¸ì„œ
    print("-"*70)
    print("[ì˜ˆì‹œ 2] ê¸°ìˆ  ë¬¸ì„œ")
    print("-"*70)
    tech_text = [
        "ë°•ì§€ì„± ê¸°ì = ì„œìš¸ì‹œê°€ 2025ë…„ ìŠ¤ë§ˆíŠ¸ì‹œí‹° í”„ë¡œì íŠ¸ë¥¼ ë³¸ê²© ì¶”ì§„í•œë‹¤.",
        "ì´ë²ˆ í”„ë¡œì íŠ¸ëŠ” ì´ 5000ì–µì›ì˜ ì˜ˆì‚°ì´ íˆ¬ì…ëœë‹¤.",
        "ì¸ê³µì§€ëŠ¥, IoT, ë¹…ë°ì´í„° ê¸°ìˆ ì„ í™œìš©í•´ êµí†µ, í™˜ê²½, ì•ˆì „ ë¶„ì•¼ë¥¼ ê°œì„ í•  ê³„íšì´ë‹¤.",
        "ì—°ë½ì²˜: park@seoul.go.kr",
        "ì‹œë¯¼ë“¤ì˜ ì‚¶ì˜ ì§ˆ í–¥ìƒì´ ê¸°ëŒ€ëœë‹¤."
    ]
    
    print("\n[ì›ë³¸]")
    for i, line in enumerate(tech_text, 1):
        print(f"  {i}. {line}")
    
    summary = summarize_text(tech_text, model, tokenizer, device)
    print(f"\n[ìš”ì•½ë¬¸]\n  {summary}\n")
    
    # ì˜ˆì‹œ 3: ê°„ë‹¨í•œ ë¬¸ì¥
    print("-"*70)
    print("[ì˜ˆì‹œ 3] ì •ì œëœ í…ìŠ¤íŠ¸ (ì „ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ ìš”ì•½)")
    print("-"*70)
    simple_text = "ë”¥ëŸ¬ë‹ì€ ì¸ê³µì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì´ë‹¤. ë‹¤ì¸µ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ í†µí•´ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ì´ë¯¸ì§€ ì¸ì‹, ìŒì„± ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬ ë“±ì— í™œìš©ëœë‹¤."
    
    print(f"\n[ì›ë³¸]\n  {simple_text}")
    
    summary = summarize_text(simple_text, model, tokenizer, device)
    print(f"\n[ìš”ì•½ë¬¸]\n  {summary}\n")
    
    print("="*70)
    
    # ì‚¬ìš©ì ì •ì˜ í…ìŠ¤íŠ¸ ì…ë ¥ ì˜µì…˜
    print("\n[3ë‹¨ê³„] ì§ì ‘ í…ìŠ¤íŠ¸ ì…ë ¥í•˜ê¸°")
    print("-"*70)
    print("ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("ì—¬ëŸ¬ ì¤„ì„ ì…ë ¥í•˜ë ¤ë©´ ê° ì¤„ì„ ì…ë ¥ í›„ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    print("ì…ë ¥ì„ ë§ˆì¹˜ë ¤ë©´ ë¹ˆ ì¤„ì—ì„œ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    print("(ê±´ë„ˆë›°ë ¤ë©´ ë°”ë¡œ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”)\n")
    
    user_lines = []
    print("í…ìŠ¤íŠ¸ ì…ë ¥:")
    while True:
        line = input()
        if line.strip() == "":
            break
        user_lines.append(line)
    
    if user_lines:
        print("\n[ì…ë ¥í•œ í…ìŠ¤íŠ¸]")
        for i, line in enumerate(user_lines, 1):
            print(f"  {i}. {line}")
        
        summary = summarize_text(user_lines, model, tokenizer, device)
        print(f"\n[ìƒì„±ëœ ìš”ì•½ë¬¸]\n  {summary}\n")
    else:
        print("\nì…ë ¥ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.\n")
    
    print("="*70)
    print(" "*25 + "í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("="*70)
    print("\nğŸ’¡ Tip: ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ê±°ë‚˜")
    print("        API ì„œë²„ë¡œ ë§Œë“¤ì–´ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()