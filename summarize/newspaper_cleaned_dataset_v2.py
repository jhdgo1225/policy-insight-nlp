import json
import os
import requests
from konlpy.tag import Okt
import asyncio
import time
import re
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import itertools
import multiprocessing as mp
import threading

import torch
from sklearn.model_selection import train_test_split
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import matplotlib.pyplot as plt

# tokenizers ë³‘ë ¬ ì²˜ë¦¬ ì¶©ëŒ ë°©ì§€
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ========================================
# Singleton Tokenizer with Thread Safety
# ========================================
class TokenizerSingleton:
    """ìŠ¤ë ˆë“œ ì„¸ì´í”„ ì‹±ê¸€í†¤ í† í¬ë‚˜ì´ì €"""
    _instance = None
    _lock = threading.Lock()
    _tokenizer = None
    _model_ckpt = None
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(TokenizerSingleton, cls).__new__(cls)
        return cls._instance
    
    def get_tokenizer(self, model_ckpt='gogamza/kobart-base-v2'):
        """í† í¬ë‚˜ì´ì €ë¥¼ ë°˜í™˜ (í•„ìš”ì‹œ ì´ˆê¸°í™”)"""
        if self._tokenizer is None or self._model_ckpt != model_ckpt:
            with self._lock:
                # Double-checked locking
                if self._tokenizer is None or self._model_ckpt != model_ckpt:
                    print(f"í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘: {model_ckpt}")
                    self._tokenizer = AutoTokenizer.from_pretrained(model_ckpt, force_download=True)
                    self._model_ckpt = model_ckpt
                    print("í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ!")
        return self._tokenizer

# ì „ì—­ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤
tokenizer_singleton = TokenizerSingleton()

def tokenize_summarize_dataset(dataset, model_ckpt='gogamza/kobart-base-v2'):
    tokenizer = tokenizer_singleton.get_tokenizer(model_ckpt)
    body_tokens = []
    summarize_tokens = []
    for data in dataset:
        body_tokens.append(tokenizer(data['body']))
        summarize_tokens.append(tokenizer(data['summarize']))
    return {'body': body_tokens, 'summarize': summarize_tokens}


def filter_text(lines):
    """
    ê¸°ì/ì´ë©”ì¼ í¬í•¨ ë¬¸ì¥ ì œê±°
    """
    reporter_pattern = r'[ê°€-í£]{2,4}\s+ê¸°ì'
    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'

    def should_keep(segment):
        """ê¸°ì/ì´ë©”ì¼ì´ ì—†ìœ¼ë©´ True"""
        return (not re.search(reporter_pattern, segment) and 
                not re.search(email_pattern, segment))

    return [line for line in lines if should_keep(line)]

def cleanse_single_article_async(data_tuple):
    """ë‹¨ì¼ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ë¹„ë™ê¸° ì²˜ë¦¬ìš©)"""
    article, idx = data_tuple
    
    print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì‘ì—… ì¤‘...")
    
    try:
        # ì›ë³¸ ë°ì´í„° ë³µì‚¬í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        article_copy = article.copy()
        filtered_body_sentences = filter_text(article_copy['body'])
        
        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²´í¬
        if not filtered_body_sentences:
            article_copy['body'] = ""
            print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì™„ë£Œ (ë¹ˆ ë³¸ë¬¸)")
            return idx, article_copy
        
        tokenizer = tokenizer_singleton.get_tokenizer('gogamza/kobart-base-v2')
        tokenized = tokenizer(filtered_body_sentences)
        
        # input_idsê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not tokenized.get('input_ids'):
            article_copy['body'] = ""
            print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì™„ë£Œ (í† í°í™” ì‹¤íŒ¨)")
            return idx, article_copy
        
        # í† í° ê¸¸ì´ ê³„ì‚°
        total_tokens = len(list(itertools.chain(*tokenized['input_ids'])))
        article_copy['body'] = filtered_body_sentences if total_tokens <= 1024 else ""
        
        print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì‘ì—… ì™„ë£Œ! (í† í° ìˆ˜: {total_tokens})")
        return idx, article_copy
        
    except (IndexError, KeyError, TypeError) as e:
        print(f"âš ï¸  {idx + 1}ë²ˆ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {str(e)}")
        article_copy = article.copy()
        article_copy['body'] = ""  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë³¸ë¬¸ìœ¼ë¡œ ì„¤ì •
        return idx, article_copy
    except Exception as e:
        print(f"âŒ {idx + 1}ë²ˆ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {str(e)}")
        article_copy = article.copy()
        article_copy['body'] = ""  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë³¸ë¬¸ìœ¼ë¡œ ì„¤ì •
        return idx, article_copy

async def process_batch_async(batch, semaphore):
    """ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•´ ë™ì‹œ ì‹¤í–‰ ìˆ˜ë¥¼ ì œí•œí•˜ë©´ì„œ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    async with semaphore:
        # asyncio.to_threadë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ (JVM ì•ˆì „)
        tasks = [asyncio.to_thread(cleanse_single_article_async, article_data) for article_data in batch]
        return await asyncio.gather(*tasks)
        

async def cleanse_articles_async_batch(dataset, batch_size=50, max_concurrent=5):
    """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëª¨ë“  ê¸°ì‚¬ë¥¼ í´ë¦°ì§•í•©ë‹ˆë‹¤."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # (article, idx) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_with_idx = [(dataset[idx], idx) for idx in range(len(dataset))]
    
    # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batches = [articles_with_idx[i:i + batch_size] for i in range(0, len(articles_with_idx), batch_size)]
    
    print(f"ì´ {len(batches)}ê°œì˜ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë°°ì¹˜ í¬ê¸°: {batch_size}, ìµœëŒ€ ë™ì‹œ ì‹¤í–‰: {max_concurrent})")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}ê°œ")
    
    # ë°°ì¹˜ë³„ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
    all_results = []
    for i, batch in enumerate(batches):
        print(f"ë°°ì¹˜ {i+1}/{len(batches)} ì²˜ë¦¬ ì¤‘...")
        batch_results = await process_batch_async(batch, semaphore)
        all_results.extend(batch_results)
    
    # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    all_results.sort(key=lambda x: x[0])  # idxë¡œ ì •ë ¬
    
    return [result[1] for result in all_results]  # articleë§Œ ë°˜í™˜

def cleanse_single_article_thread(data_tuple):
    """ë‹¨ì¼ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    import requests
    from konlpy.tag import Okt
    
    article, idx = data_tuple
    
    # ê° í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ë¶ˆìš©ì–´ì™€ okt ì´ˆê¸°í™”
    try:
        url = "https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.txt"
        korean_stopwords = set(requests.get(url).text.split("\n"))
        for stopword in ["ì€", "ëŠ”", "ì´", "ê°€", "ê³ ", "ì •ë§", "ì˜", "ì´ë‚˜", "ì´ë¼ê³ ", "ì¸", "ì´ë‹¤", "í•˜ì—¬", "``", "ì—", "ì—ëŠ”"]:
            korean_stopwords.add(stopword) 
        korean_stopwords.remove("ëª¨")
    except:
        # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë¶ˆìš©ì–´ ì‚¬ìš©
        korean_stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ê³ ", "ì •ë§", "ì˜", "ì´ë‚˜", "ì´ë¼ê³ ", "ì¸", "ì´ë‹¤", "í•˜ì—¬", "``", "ì—", "ì—ëŠ”"}
    
    okt = Okt()
    
    def clean_text_local(text):
        if not isinstance(text, str):
            return ""

        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
        splited_texts = text.split()
        okt_text = okt.morphs(text)
        cleaned_texts = []
        temp = ""
        pointer = 0
        for split_text in splited_texts:
            while pointer < len(okt_text) and okt_text[pointer] in split_text:
                # ë¶ˆìš©ì–´ ì œê±°
                if not okt_text[pointer] in korean_stopwords:
                    temp += okt_text[pointer]
                pointer += 1
            if temp: cleaned_texts.append(temp)
            temp = ""
        return " ".join(cleaned_texts)
    
    print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì‘ì—… ì¤‘...")
    
    try:
        # ì›ë³¸ ë°ì´í„° ë³µì‚¬í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        article_copy = article.copy()
        filtered_body_sentences = filter_text(article_copy['body'])
        
        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²´í¬
        if not filtered_body_sentences:
            article_copy['body'] = ""
            print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì™„ë£Œ (ë¹ˆ ë³¸ë¬¸)")
            return idx, article_copy
        
        tokenizer = tokenizer_singleton.get_tokenizer('gogamza/kobart-base-v2')
        tokenized = tokenizer(filtered_body_sentences)
        
        # input_idsê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not tokenized.get('input_ids'):
            article_copy['body'] = ""
            print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì™„ë£Œ (í† í°í™” ì‹¤íŒ¨)")
            return idx, article_copy
        
        # í† í° ê¸¸ì´ ê³„ì‚°
        total_tokens = len(list(itertools.chain(*tokenized['input_ids'])))
        article_copy['body'] = filtered_body_sentences if total_tokens <= 1024 else ""
        
        print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì‘ì—… ì™„ë£Œ! (í† í° ìˆ˜: {total_tokens})")
        return idx, article_copy
        
    except (IndexError, KeyError, TypeError) as e:
        print(f"âš ï¸  {idx + 1}ë²ˆ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {str(e)}")
        article_copy = article.copy()
        article_copy['body'] = ""  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë³¸ë¬¸ìœ¼ë¡œ ì„¤ì •
        return idx, article_copy
    except Exception as e:
        print(f"âŒ {idx + 1}ë²ˆ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {str(e)}")
        article_copy = article.copy()
        article_copy['body'] = ""  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë³¸ë¬¸ìœ¼ë¡œ ì„¤ì •
        return idx, article_copy

async def process_batch_async(batch, semaphore):
    """ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•´ ë™ì‹œ ì‹¤í–‰ ìˆ˜ë¥¼ ì œí•œí•˜ë©´ì„œ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    async with semaphore:
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•´ CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:
            tasks = [
                loop.run_in_executor(executor, cleanse_single_article_async, article_data)
                for article_data in batch
            ]
            return await asyncio.gather(*tasks)

async def cleanse_articles_async_batch(dataset, batch_size=50, max_concurrent=5):
    """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëª¨ë“  ê¸°ì‚¬ë¥¼ í´ë¦°ì§•í•©ë‹ˆë‹¤."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # (article, idx) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_with_idx = [(dataset[idx], idx) for idx in range(len(dataset))]
    
    # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batches = [articles_with_idx[i:i + batch_size] for i in range(0, len(articles_with_idx), batch_size)]
    
    print(f"ì´ {len(batches)}ê°œì˜ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë°°ì¹˜ í¬ê¸°: {batch_size}, ìµœëŒ€ ë™ì‹œ ì‹¤í–‰: {max_concurrent})")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}ê°œ")
    
    # ë°°ì¹˜ë³„ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
    all_results = []
    for i, batch in enumerate(batches):
        print(f"ë°°ì¹˜ {i+1}/{len(batches)} ì²˜ë¦¬ ì¤‘...")
        batch_results = await process_batch_async(batch, semaphore)
        all_results.extend(batch_results)
    
    # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    all_results.sort(key=lambda x: x[0])  # idxë¡œ ì •ë ¬
    
    return [result[1] for result in all_results]  # articleë§Œ ë°˜í™˜

def cleanse_articles_parallel_process(dataset):
    """í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ê¸°ì‚¬ë¥¼ í´ë¦°ì§• (ë¡œì»¬ í™˜ê²½ìš©)"""
    # CPU ì§‘ì•½ì  ì‘ì—…(í˜•íƒœì†Œ ë¶„ì„, í…ìŠ¤íŠ¸ ì²˜ë¦¬)ì´ë¯€ë¡œ ProcessPoolExecutor ì‚¬ìš©
    max_workers = min(mp.cpu_count(), len(dataset))  # CPU ì½”ì–´ ìˆ˜ë§Œí¼

    print(f"í”„ë¡œì„¸ìŠ¤ í’€ í¬ê¸°: {max_workers}ê°œ")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}ê°œ")

    # (article, idx) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_with_idx = [(dataset[idx], idx) for idx in range(len(dataset))]

    # ProcessPoolExecutor ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        results = list(executor.map(cleanse_single_article_thread, articles_with_idx))

    # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[0])  # idxë¡œ ì •ë ¬

    return [result[1] for result in results]  # articleë§Œ ë°˜í™˜

async def cleanse_articles_parallel_thread(dataset):
    """ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ê¸°ì‚¬ë¥¼ í´ë¦°ì§• (ë…¸íŠ¸ë¶ í™˜ê²½ìš©)"""
    # I/O ì§‘ì•½ì  ì‘ì—…(requests, file I/O)ì´ ë§ìœ¼ë¯€ë¡œ ThreadPoolExecutor ì‚¬ìš©
    max_workers = min(mp.cpu_count() * 2, len(dataset), 8)  # CPU ì½”ì–´ì˜ 2ë°°ê¹Œì§€, ìµœëŒ€ 8ê°œ

    print(f"ìŠ¤ë ˆë“œ í’€ í¬ê¸°: {max_workers}ê°œ")

    # (article, idx) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_with_idx = [(dataset[idx], idx) for idx in range(len(dataset))]

    # asyncioì˜ run_in_executorë¥¼ ì‚¬ìš©í•´ ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬
    loop = asyncio.get_event_loop()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        tasks = [
            loop.run_in_executor(executor, cleanse_single_article_thread, article_data)
            for article_data in articles_with_idx
        ]

        # ëª¨ë“  ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
        results = await asyncio.gather(*tasks)

    # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[0])  # idxë¡œ ì •ë ¬

    return [result[1] for result in results]  # articleë§Œ ë°˜í™˜

# í™˜ê²½ì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
async def cleanse_articles_parallel(dataset, model_ckpt=None, use_async_batch=True):
    """í™˜ê²½ì— ë§ëŠ” ë³‘ë ¬ ì²˜ë¦¬ ì„ íƒ"""
    if use_async_batch:
        # ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© (ê°€ì¥ ë¹ ë¦„)
        return await cleanse_articles_async_batch(dataset, batch_size=50, max_concurrent=mp.cpu_count())
    else:
        # í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
        return cleanse_articles_parallel_process(dataset)

async def main_async():
    """ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜"""
    # ========================================
    # 1. ì„¤ì • ë° ì´ˆê¸°í™”
    # ========================================
    # print("Okt ë° ë¶ˆìš©ì–´ ì´ˆê¸°í™” ì¤‘...")
    # JVM ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë¨¼ì € ì „ì—­ Okt ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
    # get_okt_and_stopwords()
    # print("Okt ì´ˆê¸°í™” ì™„ë£Œ!")
    
    model_ckpt = 'gogamza/kobart-base-v2'

    # ========================================
    # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    # ========================================

    with open('./newspaper_summarize_jsonl/newspaper_summarize.jsonl') as f:
        dataset = [json.loads(line) for line in f]
    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)
    valid_dataset, test_dataset = train_test_split(test_dataset, test_size=0.5, random_state=42, shuffle=True)

    # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()

    print("ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¡œ í›ˆë ¨ ë°ì´í„°ì…‹ í´ë¦°ì§• ì‹œì‘...")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}ê°œ")
    print(f"ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜: {len(train_dataset)}ê°œ")

    # 2. ê²€ì¦ ë°ì´í„°ì…‹ ì²˜ë¦¬
    print("\n=== ê²€ì¦ ë°ì´í„°ì…‹ í´ë¦°ì§• ì‹œì‘ ===")
    valid_start_time = time.time()
    cleaned_valid_dataset = await cleanse_articles_parallel(valid_dataset, model_ckpt, use_async_batch=True)
    cleaned_valid_dataset = [dataset for dataset in cleaned_valid_dataset if len(dataset['body']) > 0]
    valid_end_time = time.time()
    valid_processing_time = valid_end_time - valid_start_time
    print(f"ê²€ì¦ ë°ì´í„°ì…‹ í´ë¦°ì§• ì™„ë£Œ! ì†Œìš” ì‹œê°„: {valid_processing_time:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(cleaned_valid_dataset)}ê°œ")
    print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {len(cleaned_valid_dataset)/valid_processing_time:.2f}ê°œ/ì´ˆ")

    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('./cleaned_datasets_v2', exist_ok=True)

    with open('./cleaned_datasets_v2/cleaned_valid_dataset.json', 'w', encoding='utf-8') as f:
        json.dump(cleaned_valid_dataset, f, ensure_ascii=False, indent=2)
    print(f"âœ… í´ë¦°ì§•ëœ ê²€ì¦ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {len(cleaned_valid_dataset)}ê°œ ê¸°ì‚¬")

    # 1. í›ˆë ¨ ë°ì´í„°ì…‹ ì²˜ë¦¬
    print("\n=== í›ˆë ¨ ë°ì´í„°ì…‹ í´ë¦°ì§• ì‹œì‘ ===")
    cleaned_train_dataset = await cleanse_articles_parallel(train_dataset, model_ckpt, use_async_batch=True)
    cleaned_train_dataset = [dataset for dataset in cleaned_train_dataset if len(dataset['body']) > 0]
    
    # ì²˜ë¦¬ ì‹œê°„ ì¶œë ¥
    end_time = time.time()
    processing_time = end_time - start_time
    print(f"í›ˆë ¨ ë°ì´í„°ì…‹ í´ë¦°ì§• ì™„ë£Œ! ì†Œìš” ì‹œê°„: {processing_time:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(cleaned_train_dataset)}ê°œ")
    print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {len(cleaned_train_dataset)/processing_time:.2f}ê°œ/ì´ˆ")
    
    # í´ë¦°ì§•ëœ ë°ì´í„°ì…‹ ì €ì¥
    with open('./cleaned_datasets_v2/cleaned_train_dataset.json', 'w', encoding='utf-8') as f:
        json.dump(cleaned_train_dataset, f, ensure_ascii=False, indent=2)
    print(f"âœ… í´ë¦°ì§•ëœ í›ˆë ¨ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {len(cleaned_train_dataset)}ê°œ ê¸°ì‚¬")

    # 3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì²˜ë¦¬
    print("\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë¦°ì§• ì‹œì‘ ===")
    test_start_time = time.time()
    cleaned_test_dataset = await cleanse_articles_parallel(test_dataset, model_ckpt, use_async_batch=True)
    cleaned_test_dataset = [dataset for dataset in cleaned_test_dataset if len(dataset['body']) > 0]
    test_end_time = time.time()
    test_processing_time = test_end_time - test_start_time
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë¦°ì§• ì™„ë£Œ! ì†Œìš” ì‹œê°„: {test_processing_time:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(cleaned_test_dataset)}ê°œ")
    print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {len(cleaned_test_dataset)/test_processing_time:.2f}ê°œ/ì´ˆ")

    with open('./cleaned_datasets_v2/cleaned_test_dataset.json', 'w', encoding='utf-8') as f:
        json.dump(cleaned_test_dataset, f, ensure_ascii=False, indent=2)
    print(f"âœ… í´ë¦°ì§•ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {len(cleaned_test_dataset)}ê°œ ê¸°ì‚¬")

    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„
    total_processing_time = processing_time + valid_processing_time + test_processing_time
    total_articles = len(cleaned_train_dataset) + len(cleaned_valid_dataset) + len(cleaned_test_dataset)
    print(f"\n=== ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ===")
    print(f"ì „ì²´ ì†Œìš” ì‹œê°„: {total_processing_time:.2f}ì´ˆ")
    print(f"ì „ì²´ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {total_articles}ê°œ")
    print(f"ì „ì²´ í‰ê·  ì²˜ë¦¬ ì†ë„: {total_articles/total_processing_time:.2f}ê°œ/ì´ˆ")

    # ë°ì´í„°ì…‹ í¬ê¸° ì •ë³´ ì €ì¥
    dataset_info = {
        "train_size": len(cleaned_train_dataset),
        "valid_size": len(cleaned_valid_dataset),
        "test_size": len(cleaned_test_dataset),
        "total_size": total_articles,
        "processing_time": {
            "train_time": processing_time,
            "valid_time": valid_processing_time,
            "test_time": test_processing_time,
            "total_time": total_processing_time
        },
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    with open('./cleaned_datasets_v2/dataset_info.json', 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=2)
    print(f"âœ… ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥ ì™„ë£Œ")
    
    print(f"\nğŸ‰ ëª¨ë“  ì²˜ë¦¬ ë° ì €ì¥ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: ./cleaned_datasets_v2/")
    print(f"ğŸ“Š ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.2f}ì´ˆ")
    
    return cleaned_train_dataset, cleaned_valid_dataset, cleaned_test_dataset


if __name__ == "__main__":
    # ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    print("=== ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
    asyncio.run(main_async())
