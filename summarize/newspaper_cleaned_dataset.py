import json
import os
import requests
from konlpy.tag import Okt
import asyncio
import time
import re
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp

import torch
from sklearn.model_selection import train_test_split
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import matplotlib.pyplot as plt

# tokenizers ë³‘ë ¬ ì²˜ë¦¬ ì¶©ëŒ ë°©ì§€
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ì „ì—­ ë³€ìˆ˜ë¡œ Oktì™€ ë¶ˆìš©ì–´ë¥¼ í•œ ë²ˆë§Œ ì´ˆê¸°í™” (JVM ì¶©ëŒ ë°©ì§€)
_okt_instance = None
_korean_stopwords = None

def get_okt_and_stopwords():
    """ì „ì—­ Okt ì¸ìŠ¤í„´ìŠ¤ì™€ ë¶ˆìš©ì–´ë¥¼ ë°˜í™˜ (JVM ì¶©ëŒ ë°©ì§€)"""
    global _okt_instance, _korean_stopwords
    
    if _okt_instance is None:
        from konlpy.tag import Okt
        _okt_instance = Okt()
        
    if _korean_stopwords is None:
        try:
            import requests
            url = "https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.txt"
            _korean_stopwords = set(requests.get(url).text.split("\n"))
            for stopword in ["ì€", "ëŠ”", "ì´", "ê°€", "ê³ ", "ì •ë§", "ì˜", "ì´ë‚˜", "ì´ë¼ê³ ", "ì¸", "ì´ë‹¤", "í•˜ì—¬", "``", "ì—", "ì—ëŠ”"]:
                _korean_stopwords.add(stopword) 
            _korean_stopwords.remove("ëª¨")
        except:
            # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë¶ˆìš©ì–´ ì‚¬ìš©
            _korean_stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ê³ ", "ì •ë§", "ì˜", "ì´ë‚˜", "ì´ë¼ê³ ", "ì¸", "ì´ë‹¤", "í•˜ì—¬", "``", "ì—", "ì—ëŠ”"}
    
    return _okt_instance, _korean_stopwords


def tokenize_summarize_dataset(dataset, model_ckpt='gogamza/kobart-base-v2'):
    tokenizer = AutoTokenizer.from_pretrained(model_ckpt, force_download=True)
    body_tokens = []
    summarize_tokens = []
    for data in dataset:
        body_tokens.append(tokenizer(data['body']))
        summarize_tokens.append(tokenizer(data['summarize']))
    return {'body': body_tokens, 'summarize': summarize_tokens}


def filter_text(lines):
    """
    ê¸°ì/ì´ë©”ì¼ í¬í•¨ ë¬¸ì¥ ì œê±°
    """
    reporter_pattern = r'[ê°€-í£]{2,4}\s+ê¸°ì'
    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'

    def should_keep(segment):
        """ê¸°ì/ì´ë©”ì¼ì´ ì—†ìœ¼ë©´ True"""
        return (not re.search(reporter_pattern, segment) and 
                not re.search(email_pattern, segment))

    return [line for line in lines if should_keep(line)]

def cleanse_single_article_async(data_tuple):
    """ë‹¨ì¼ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ë¹„ë™ê¸° ì²˜ë¦¬ìš©)"""
    article, idx = data_tuple
    
    # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (JVM ì¶©ëŒ ë°©ì§€)
    okt, korean_stopwords = get_okt_and_stopwords()
    
    def clean_text_local(text):
        if not isinstance(text, str):
            return ""

        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
        splited_texts = text.split()
        okt_text = okt.morphs(text)
        cleaned_texts = []
        temp = ""
        pointer = 0
        for split_text in splited_texts:
            while pointer < len(okt_text) and okt_text[pointer] in split_text:
                # ë¶ˆìš©ì–´ ì œê±°
                if not okt_text[pointer] in korean_stopwords:
                    temp += okt_text[pointer]
                pointer += 1
            if temp: cleaned_texts.append(temp)
            temp = ""
        return " ".join(cleaned_texts)
    
    print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì‘ì—… ì¤‘...")
    
    # ì›ë³¸ ë°ì´í„° ë³µì‚¬í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    article_copy = article.copy()
    filtered_body_sentences = filter_text(article_copy['body']) 
    cleaned_body = [clean_text_local(sentence) for sentence in filtered_body_sentences]
    article_copy['body'] = cleaned_body
    
    print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì‘ì—… ì™„ë£Œ!")
    return idx, article_copy

async def process_batch_async(batch, semaphore):
    """ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•´ ë™ì‹œ ì‹¤í–‰ ìˆ˜ë¥¼ ì œí•œí•˜ë©´ì„œ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    async with semaphore:
        # asyncio.to_threadë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ (JVM ì•ˆì „)
        tasks = [asyncio.to_thread(cleanse_single_article_async, article_data) for article_data in batch]
        return await asyncio.gather(*tasks)
        

async def cleanse_articles_async_batch(dataset, batch_size=50, max_concurrent=5):
    """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëª¨ë“  ê¸°ì‚¬ë¥¼ í´ë¦°ì§•í•©ë‹ˆë‹¤."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # (article, idx) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_with_idx = [(dataset[idx], idx) for idx in range(len(dataset))]
    
    # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batches = [articles_with_idx[i:i + batch_size] for i in range(0, len(articles_with_idx), batch_size)]
    
    print(f"ì´ {len(batches)}ê°œì˜ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë°°ì¹˜ í¬ê¸°: {batch_size}, ìµœëŒ€ ë™ì‹œ ì‹¤í–‰: {max_concurrent})")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}ê°œ")
    
    # ë°°ì¹˜ë³„ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
    all_results = []
    for i, batch in enumerate(batches):
        print(f"ë°°ì¹˜ {i+1}/{len(batches)} ì²˜ë¦¬ ì¤‘...")
        batch_results = await process_batch_async(batch, semaphore)
        all_results.extend(batch_results)
    
    # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    all_results.sort(key=lambda x: x[0])  # idxë¡œ ì •ë ¬
    
    return [result[1] for result in all_results]  # articleë§Œ ë°˜í™˜

def cleanse_single_article_thread(data_tuple):
    """ë‹¨ì¼ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    import requests
    from konlpy.tag import Okt
    
    article, idx = data_tuple
    
    # ê° í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ë¶ˆìš©ì–´ì™€ okt ì´ˆê¸°í™”
    try:
        url = "https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.txt"
        korean_stopwords = set(requests.get(url).text.split("\n"))
        for stopword in ["ì€", "ëŠ”", "ì´", "ê°€", "ê³ ", "ì •ë§", "ì˜", "ì´ë‚˜", "ì´ë¼ê³ ", "ì¸", "ì´ë‹¤", "í•˜ì—¬", "``", "ì—", "ì—ëŠ”"]:
            korean_stopwords.add(stopword) 
        korean_stopwords.remove("ëª¨")
    except:
        # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë¶ˆìš©ì–´ ì‚¬ìš©
        korean_stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ê³ ", "ì •ë§", "ì˜", "ì´ë‚˜", "ì´ë¼ê³ ", "ì¸", "ì´ë‹¤", "í•˜ì—¬", "``", "ì—", "ì—ëŠ”"}
    
    okt = Okt()
    
    def clean_text_local(text):
        if not isinstance(text, str):
            return ""

        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
        splited_texts = text.split()
        okt_text = okt.morphs(text)
        cleaned_texts = []
        temp = ""
        pointer = 0
        for split_text in splited_texts:
            while pointer < len(okt_text) and okt_text[pointer] in split_text:
                # ë¶ˆìš©ì–´ ì œê±°
                if not okt_text[pointer] in korean_stopwords:
                    temp += okt_text[pointer]
                pointer += 1
            if temp: cleaned_texts.append(temp)
            temp = ""
        return " ".join(cleaned_texts)
    
    print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì‘ì—… ì¤‘...")
    
    # ì›ë³¸ ë°ì´í„° ë³µì‚¬í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    article_copy = article.copy()
    filtered_body_sentences = filter_text(article_copy['body']) 
    cleaned_body = [clean_text_local(sentence) for sentence in filtered_body_sentences]
    article_copy['body'] = cleaned_body
    
    print(f"{idx + 1}ë²ˆ ê¸°ì‚¬ ë³¸ë¬¸ í´ë¦°ì§• ì‘ì—… ì™„ë£Œ!")
    return idx, article_copy

async def process_batch_async(batch, semaphore):
    """ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•´ ë™ì‹œ ì‹¤í–‰ ìˆ˜ë¥¼ ì œí•œí•˜ë©´ì„œ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    async with semaphore:
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•´ CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:
            tasks = [
                loop.run_in_executor(executor, cleanse_single_article_async, article_data)
                for article_data in batch
            ]
            return await asyncio.gather(*tasks)

async def cleanse_articles_async_batch(dataset, batch_size=50, max_concurrent=5):
    """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëª¨ë“  ê¸°ì‚¬ë¥¼ í´ë¦°ì§•í•©ë‹ˆë‹¤."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # (article, idx) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_with_idx = [(dataset[idx], idx) for idx in range(len(dataset))]
    
    # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batches = [articles_with_idx[i:i + batch_size] for i in range(0, len(articles_with_idx), batch_size)]
    
    print(f"ì´ {len(batches)}ê°œì˜ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë°°ì¹˜ í¬ê¸°: {batch_size}, ìµœëŒ€ ë™ì‹œ ì‹¤í–‰: {max_concurrent})")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}ê°œ")
    
    # ë°°ì¹˜ë³„ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
    all_results = []
    for i, batch in enumerate(batches):
        print(f"ë°°ì¹˜ {i+1}/{len(batches)} ì²˜ë¦¬ ì¤‘...")
        batch_results = await process_batch_async(batch, semaphore)
        all_results.extend(batch_results)
    
    # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    all_results.sort(key=lambda x: x[0])  # idxë¡œ ì •ë ¬
    
    return [result[1] for result in all_results]  # articleë§Œ ë°˜í™˜

def cleanse_articles_parallel_process(dataset):
    """í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ê¸°ì‚¬ë¥¼ í´ë¦°ì§• (ë¡œì»¬ í™˜ê²½ìš©)"""
    # CPU ì§‘ì•½ì  ì‘ì—…(í˜•íƒœì†Œ ë¶„ì„, í…ìŠ¤íŠ¸ ì²˜ë¦¬)ì´ë¯€ë¡œ ProcessPoolExecutor ì‚¬ìš©
    max_workers = min(mp.cpu_count(), len(dataset))  # CPU ì½”ì–´ ìˆ˜ë§Œí¼

    print(f"í”„ë¡œì„¸ìŠ¤ í’€ í¬ê¸°: {max_workers}ê°œ")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}ê°œ")

    # (article, idx) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_with_idx = [(dataset[idx], idx) for idx in range(len(dataset))]

    # ProcessPoolExecutor ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        results = list(executor.map(cleanse_single_article_thread, articles_with_idx))

    # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[0])  # idxë¡œ ì •ë ¬

    return [result[1] for result in results]  # articleë§Œ ë°˜í™˜

async def cleanse_articles_parallel_thread(dataset):
    """ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ê¸°ì‚¬ë¥¼ í´ë¦°ì§• (ë…¸íŠ¸ë¶ í™˜ê²½ìš©)"""
    # I/O ì§‘ì•½ì  ì‘ì—…(requests, file I/O)ì´ ë§ìœ¼ë¯€ë¡œ ThreadPoolExecutor ì‚¬ìš©
    max_workers = min(mp.cpu_count() * 2, len(dataset), 8)  # CPU ì½”ì–´ì˜ 2ë°°ê¹Œì§€, ìµœëŒ€ 8ê°œ

    print(f"ìŠ¤ë ˆë“œ í’€ í¬ê¸°: {max_workers}ê°œ")

    # (article, idx) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_with_idx = [(dataset[idx], idx) for idx in range(len(dataset))]

    # asyncioì˜ run_in_executorë¥¼ ì‚¬ìš©í•´ ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬
    loop = asyncio.get_event_loop()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        tasks = [
            loop.run_in_executor(executor, cleanse_single_article_thread, article_data)
            for article_data in articles_with_idx
        ]

        # ëª¨ë“  ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
        results = await asyncio.gather(*tasks)

    # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[0])  # idxë¡œ ì •ë ¬

    return [result[1] for result in results]  # articleë§Œ ë°˜í™˜

# í™˜ê²½ì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
async def cleanse_articles_parallel(dataset, model_ckpt=None, use_async_batch=True):
    """í™˜ê²½ì— ë§ëŠ” ë³‘ë ¬ ì²˜ë¦¬ ì„ íƒ"""
    if use_async_batch:
        # ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© (ê°€ì¥ ë¹ ë¦„)
        return await cleanse_articles_async_batch(dataset, batch_size=50, max_concurrent=mp.cpu_count())
    else:
        # í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
        return cleanse_articles_parallel_process(dataset)

async def main_async():
    """ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜"""
    # ========================================
    # 1. ì„¤ì • ë° ì´ˆê¸°í™”
    # ========================================
    print("Okt ë° ë¶ˆìš©ì–´ ì´ˆê¸°í™” ì¤‘...")
    # JVM ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë¨¼ì € ì „ì—­ Okt ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
    get_okt_and_stopwords()
    print("Okt ì´ˆê¸°í™” ì™„ë£Œ!")
    
    model_ckpt = 'gogamza/kobart-base-v2'
    output_dir = "./kobart_korean_summarize_model"

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # ========================================
    # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    # ========================================

    with open('./newspaper_summarize_jsonl/newspaper_summarize.jsonl') as f:
        dataset = [json.loads(line) for line in f]
    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)
    valid_dataset, test_dataset = train_test_split(test_dataset, test_size=0.5, random_state=42, shuffle=True)

    # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()

    print("ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¡œ í›ˆë ¨ ë°ì´í„°ì…‹ í´ë¦°ì§• ì‹œì‘...")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}ê°œ")
    print(f"ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜: {len(train_dataset)}ê°œ")

    # 1. í›ˆë ¨ ë°ì´í„°ì…‹ ì²˜ë¦¬
    print("\n=== í›ˆë ¨ ë°ì´í„°ì…‹ í´ë¦°ì§• ì‹œì‘ ===")
    cleaned_train_dataset = await cleanse_articles_parallel(train_dataset, model_ckpt, use_async_batch=True)
    
    # ì²˜ë¦¬ ì‹œê°„ ì¶œë ¥
    end_time = time.time()
    processing_time = end_time - start_time
    print(f"í›ˆë ¨ ë°ì´í„°ì…‹ í´ë¦°ì§• ì™„ë£Œ! ì†Œìš” ì‹œê°„: {processing_time:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(cleaned_train_dataset)}ê°œ")
    print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {len(cleaned_train_dataset)/processing_time:.2f}ê°œ/ì´ˆ")

    # 2. ê²€ì¦ ë°ì´í„°ì…‹ ì²˜ë¦¬
    print("\n=== ê²€ì¦ ë°ì´í„°ì…‹ í´ë¦°ì§• ì‹œì‘ ===")
    valid_start_time = time.time()
    cleaned_valid_dataset = await cleanse_articles_parallel(valid_dataset, model_ckpt, use_async_batch=True)
    valid_end_time = time.time()
    valid_processing_time = valid_end_time - valid_start_time
    print(f"ê²€ì¦ ë°ì´í„°ì…‹ í´ë¦°ì§• ì™„ë£Œ! ì†Œìš” ì‹œê°„: {valid_processing_time:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(cleaned_valid_dataset)}ê°œ")
    print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {len(cleaned_valid_dataset)/valid_processing_time:.2f}ê°œ/ì´ˆ")

    # 3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì²˜ë¦¬
    print("\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë¦°ì§• ì‹œì‘ ===")
    test_start_time = time.time()
    cleaned_test_dataset = await cleanse_articles_parallel(test_dataset, model_ckpt, use_async_batch=True)
    test_end_time = time.time()
    test_processing_time = test_end_time - test_start_time
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë¦°ì§• ì™„ë£Œ! ì†Œìš” ì‹œê°„: {test_processing_time:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(cleaned_test_dataset)}ê°œ")
    print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {len(cleaned_test_dataset)/test_processing_time:.2f}ê°œ/ì´ˆ")

    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„
    total_processing_time = processing_time + valid_processing_time + test_processing_time
    total_articles = len(cleaned_train_dataset) + len(cleaned_valid_dataset) + len(cleaned_test_dataset)
    print(f"\n=== ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ===")
    print(f"ì „ì²´ ì†Œìš” ì‹œê°„: {total_processing_time:.2f}ì´ˆ")
    print(f"ì „ì²´ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {total_articles}ê°œ")
    print(f"ì „ì²´ í‰ê·  ì²˜ë¦¬ ì†ë„: {total_articles/total_processing_time:.2f}ê°œ/ì´ˆ")

    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('./cleaned_datasets', exist_ok=True)
    
    # í´ë¦°ì§•ëœ ë°ì´í„°ì…‹ ì €ì¥
    with open('./cleaned_datasets/cleaned_train_dataset.json', 'w', encoding='utf-8') as f:
        json.dump(cleaned_train_dataset, f, ensure_ascii=False, indent=2)
    print(f"âœ… í´ë¦°ì§•ëœ í›ˆë ¨ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {len(cleaned_train_dataset)}ê°œ ê¸°ì‚¬")
    
    with open('./cleaned_datasets/cleaned_valid_dataset.json', 'w', encoding='utf-8') as f:
        json.dump(cleaned_valid_dataset, f, ensure_ascii=False, indent=2)
    print(f"âœ… í´ë¦°ì§•ëœ ê²€ì¦ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {len(cleaned_valid_dataset)}ê°œ ê¸°ì‚¬")
    
    with open('./cleaned_datasets/cleaned_test_dataset.json', 'w', encoding='utf-8') as f:
        json.dump(cleaned_test_dataset, f, ensure_ascii=False, indent=2)
    print(f"âœ… í´ë¦°ì§•ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {len(cleaned_test_dataset)}ê°œ ê¸°ì‚¬")
    
    # ë°ì´í„°ì…‹ í¬ê¸° ì •ë³´ ì €ì¥
    dataset_info = {
        "train_size": len(cleaned_train_dataset),
        "valid_size": len(cleaned_valid_dataset),
        "test_size": len(cleaned_test_dataset),
        "total_size": total_articles,
        "processing_time": {
            "train_time": processing_time,
            "valid_time": valid_processing_time,
            "test_time": test_processing_time,
            "total_time": total_processing_time
        },
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    with open('./cleaned_datasets/dataset_info.json', 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=2)
    print(f"âœ… ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥ ì™„ë£Œ")
    
    print(f"\nğŸ‰ ëª¨ë“  ì²˜ë¦¬ ë° ì €ì¥ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: ./cleaned_datasets/")
    print(f"ğŸ“Š ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.2f}ì´ˆ")
    
    return cleaned_train_dataset, cleaned_valid_dataset, cleaned_test_dataset


if __name__ == "__main__":
    # ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    print("=== ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
    asyncio.run(main_async())
